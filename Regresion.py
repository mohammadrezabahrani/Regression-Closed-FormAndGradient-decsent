# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wIWPOU154_rATX0Amz34BDCtK3TVG-xd
"""

import pandas as pd
import numpy as np
from google.colab import files
from sklearn import preprocessing
import matplotlib.pyplot as plt
from matplotlib import pyplot
import seaborn as sns
from matplotlib.pyplot import figure
#def Read_Data():
#    fileName = files.upload()
#    dataFrame = pd.read_csv(fileName)
#    return dataFrame 

def DataToXy(Data):
  X = Data.loc[:,0]
  y = Data.loc[:,1]
  return X,y

def SpilteDataXYTrainAndTest(Data,Percentage):
  Train = Data[0:int(len(Data)*Percentage)]
  Test =  Data[int(len(Data) * Percentage):-1]
  return Train,Test

data =  pd.read_csv("sample_data/data.csv",header=None)
X,y  = DataToXy(data)
m1 = X.shape
m2 = y.shape
print(m1, m2)#number of rows in data-train and data-test

X_train,X_test = SpilteDataXYTrainAndTest(X,0.7);
y_train,y_test = SpilteDataXYTrainAndTest(y,0.7);

X_train = np.array(X_train).reshape(len(X_train),1)
X_test = np.array(X_test).reshape(len(X_test),1)
y_train = np.array(y_train).reshape(len(y_train),1)
y_test = np.array(y_test).reshape(len(y_test),1)

def Feature_scaling(X_train, X_test):
    norm = preprocessing.StandardScaler().fit(X_train) #calculate mean and variance Note: we can't touch X-Test(unseen data)
    X_train_stand = norm.transform(X_train)#Apply norm to each element of x_train
    X_test_stand = norm.transform(X_test)#Apply  norm to each element of x_test 
    return X_train_stand, X_test_stand

X_train,X_test = Feature_scaling(X_train,X_test)
y_train,y_test = Feature_scaling(y_train,y_test)
X0_train = np.ones((len(X_train), 1))# This element does not need to be normalized because all its elements are 1
X_train = np.concatenate((X0_train, X_train), axis=1) # added  bias because in the formula we need a bias Coefficient

X0_test = np.ones((len(X_test), 1))
X_test = np.concatenate((X0_test, X_test), axis=1)

def Closed_form(X_train, y_train):#Closed_form gets the exact answer the minimum theta  minimum of j(tetha)
     # Closed form solution has a Cost calculations and sometimes we have irreversible matrix
    theta = np.matmul(np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), X_train.T), y_train)
   # print(theta)
    return theta

def Gradient_decsent(X_train, y_train):# Gradient_decsent gives an approximate answer
    alpha=.0001
    theta_GD = np.zeros((2,1))
    cost = []
    for i in range(100):
        y_hat = np.matmul(theta_GD.T, X_train.T)
        cost.append((np.sum((y_hat-y_train.T)**2))/len(X_train))
        # cost.append((i+1, error))
        theta_GD[0,0] -= alpha*np.matmul((y_hat-y_train.T), X_train[:,0].reshape(len(X_train),1)) 
        theta_GD[1,0] -= alpha*np.matmul((y_hat-y_train.T), X_train[:,1].reshape(len(X_train),1))   
    return theta_GD, cost

theta_closedForm = Closed_form(X_train, y_train)
print(f'the learned parameters calculated by closed-from method:{theta_closedForm[0,0], theta_closedForm[1,0]}')
theta_GD, cost = Gradient_decsent(X_train, y_train)
print(f'the learned parameters calculated by gradient descent method:{theta_GD[0,0], theta_GD[1,0]}')

# print(theta_closedForm.shape, X_test.shape)
y_pred_train_closedForm = np.matmul(theta_closedForm.T, X_train.T)
y_pred_test_closedForm = np.matmul(theta_closedForm.T, X_test.T)
MSE_train_closedForm = (np.sum((y_pred_train_closedForm-y_train.T)**2))/len(X_train)
MSE_test_closedForm = (np.sum((y_pred_test_closedForm-y_test.T)**2))/len(X_train)
print(f'MSE for train data using closed-form method:{MSE_train_closedForm}')
print(f'MSE for test data using closed-form method:{MSE_test_closedForm}')

y_pred_train_GD = np.matmul(theta_GD.T, X_train.T)
y_pred_test_GD= np.matmul(theta_GD.T, X_test.T)
MSE_train_GD = (np.sum((y_pred_train_GD-y_train.T)**2))/len(X_train)
MSE_test_GD = (np.sum((y_pred_test_GD-y_test.T)**2))/len(X_train)
print(f'MSE for train data using gradient descent method:{MSE_train_GD}')
print(f'MSE for test data using gradient descent method:{MSE_test_GD}')

print('cost' , cost)

pyplot.plot(range(100), cost)
plt.title("cost function")
plt.xlabel("iteration")
plt.ylabel("mean squared error")
plt.show()
pyplot.show()

def plotOfData(plotx,ploty,scatterx,scattery,title,ColorOfLine,ColorOfData):
  plt.plot(plotx,ploty,color=ColorOfLine, label=title)
  plt.legend(loc='upper left')
  plt.xlabel("X")
  plt.ylabel("Y")
  plt.scatter(scatterx,scattery, color = ColorOfData)
  plt.show()


plotOfData(X_train[:,1],theta_closedForm[0] + theta_closedForm[1]*X_train[:,1],X_train[:,1],y_train,'Train regression line (colsed-form)','black', '#88c999')
plotOfData(X_test[:,1],theta_closedForm[0] + theta_closedForm[1]*X_test[:,1],X_test[:,1],y_test,'Test regression line (colsed-form)','black','b')
plotOfData(X_train[:,1], theta_GD[0] + theta_GD[1]*X_train[:,1],X_train[:,1],y_train,'Train regression line (gradient descent)','red','#88c999')
plotOfData(X_test[:,1], theta_GD[0] + theta_GD[1]*X_test[:,1],X_test[:,1],y_test,'Test regression line (gradient descent)','red','b')